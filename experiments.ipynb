{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from  feature_engineering import get_feature_df, create_feature_bins, create_network_graph\n",
    "from data_preprocessing import get_time_frame_data, select_columns, split_network_train_set, remove_sinlge_occurences, keep_classes, get_subset_exclude_ids, get_subset_include_ids\n",
    "from constants import START_DATE, END_DATE, CUTOFF_NEGATIVE_DURATION_PERCENTAGE, CLASSES_TO_USE\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import operator\n",
    "import time\n",
    "import os\n",
    "from xgboost import XGBClassifier\n",
    "from dateutil.relativedelta import relativedelta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test gap (static and resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPERIMENT Static Gap\n",
    "def static_gap(df_portcalls, results_directory):\n",
    "    # create necessary files for logging\n",
    "    current_results_directory = results_directory + \"experiment_static_gap/\"\n",
    "    if not os.path.isdir(current_results_directory):\n",
    "        os.mkdir(current_results_directory)\n",
    "\n",
    "\n",
    "    # Retrieve the desired timeframe\n",
    "    df = get_time_frame_data(df_portcalls, START_DATE, END_DATE, \"ATA_LT\")\n",
    "\n",
    "\n",
    "    #   only keep relevant columns and set the id column\n",
    "    relevant_columns = {\"ATA_LT\": \"datetime64[ns]\", \n",
    "                    \"ATD_LT\": \"datetime64[ns]\",\n",
    "                        \"Port Name\": \"string\",\n",
    "                        \"IMO Number\": \"string\",\n",
    "                        \"(ATA) Ship Type Description\": \"string\"\n",
    "                        }\n",
    "    new_column_names = [\"Arrival Time\", \"Departure Time\", \"Port Name\", \"IMO number\", \"Ship Type\"]\n",
    "\n",
    "    id_column= \"IMO number\"\n",
    "\n",
    "    df = select_columns(df,relevant_columns=relevant_columns, new_column_names=new_column_names)\n",
    "\n",
    "    #   only keep the classes we want to classify\n",
    "    df = keep_classes(df, classes_to_keep=CLASSES_TO_USE, target_column='Ship Type', id_column='IMO number')\n",
    "   \n",
    "   \n",
    "\n",
    "    #   remove single occurences from network set\n",
    "    df_classify = remove_sinlge_occurences(df, column_name=\"IMO number\")\n",
    "\n",
    "\n",
    "    batch_size = relativedelta(months=1)\n",
    "    encoding_size = relativedelta(months=12)\n",
    "\n",
    "    for run in range(30):\n",
    "        start_network = time.time()\n",
    "\n",
    "        iteration_directory = current_results_directory + f'/run_{run+1}/'\n",
    "        if not os.path.isdir(iteration_directory):\n",
    "            os.mkdir(iteration_directory)\n",
    "\n",
    "        current_run_log = iteration_directory + 'experiment_log.txt'\n",
    "\n",
    "        f = open(current_run_log, 'a')\n",
    "\n",
    "\n",
    "        end_date_train = datetime.datetime(2018, 4, 1) + (run * relativedelta(months=1))\n",
    "        start_date_test = datetime.datetime(2018, 4, 1, microsecond=1) + (run * relativedelta(months=1))\n",
    "        \n",
    "        if start_date_test >= datetime.datetime(2019, 11, 1, microsecond=1):\n",
    "            end_date_train = end_date_train + relativedelta(months=3)\n",
    "            start_date_test = start_date_test + relativedelta(months=3)\n",
    "        \n",
    "        if start_date_test + batch_size <= END_DATE:\n",
    "            end_date_test = start_date_test + batch_size\n",
    "        else:\n",
    "            end_date_test = END_DATE\n",
    "\n",
    "        test_sample = get_time_frame_data(df_classify, start_date_test, end_date_test, \"Arrival Time\")\n",
    "        test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.1)\n",
    "        test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "        f.write(f\"start date test sample: {start_date_test} and end date test sample: {end_date_test}\\n\")\n",
    "        f.write(f'Amount of imos in test sample {len(test_imos)}\\n')        \n",
    "\n",
    "        test_batch_12 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=12), end_date=end_date_test)\n",
    "        f.write(f\"start date test 12: {end_date_test - relativedelta(months=12)} and end date test 12: {end_date_test}\\n\")\n",
    "        imos_12 = pd.unique(test_batch_12[\"IMO number\"])\n",
    "\n",
    "        f.write(f'in test there are {len(imos_12)} imos and {len(test_batch_12)} portcalls\\n')\n",
    "\n",
    "\n",
    "        for x in range(20):\n",
    "            gap_size = x * batch_size\n",
    "            f.write(f'train batch {x+1}:\\n')\n",
    "            f.write(f'gap size : {x} months')\n",
    "            # account for nov/dec 2019\n",
    "            if (end_date_train - (x * batch_size) == datetime.datetime(2019, 12, 1)) or (end_date_train - (x * batch_size) == datetime.datetime(2020, 1, 1)):\n",
    "                f.write(\"ACCOUNTING FOR THE DATA LOSS IN NOVEMBER/DECEMBER 2019 \\n \")\n",
    "                train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=datetime.datetime(2019, 11, 1) - batch_size , end_date=datetime.datetime(2019, 11, 1))\n",
    "                train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "                f.write(f\"start date train sample: {datetime.datetime(2019, 11, 1) - batch_size} and end date train sample: {datetime.datetime(2019, 11, 1)}\\n\")\n",
    "                f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "                \n",
    "                train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=datetime.datetime(2019, 11, 1) - encoding_size, end_date=datetime.datetime(2019, 11, 1))\n",
    "\n",
    "                train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "                f.write(f\"start date train: {datetime.datetime(2019, 11, 1) - encoding_size} and end date train: {datetime.datetime(2019, 11, 1)}\\n\")\n",
    "                f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "                overlap = np.intersect1d(train_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                overlap = np.intersect1d(train_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                f.write('-----\\n')\n",
    "\n",
    "            else: \n",
    "                train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=end_date_train - ((1+x) * batch_size) , end_date=end_date_train - (x * batch_size))\n",
    "                train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "                f.write(f\"start date train sample: {end_date_train - ((1+x) * batch_size)} and end date train sample: {end_date_train - (x * batch_size)}\\n\")\n",
    "                f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "                \n",
    "                train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=end_date_train - (x * batch_size) - encoding_size, end_date=end_date_train - (x * batch_size))\n",
    "\n",
    "                train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "                f.write(f\"start date train: {end_date_train - (x * batch_size) - encoding_size} and end date train: {end_date_train - (x * batch_size)}\\n\")\n",
    "                f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "                overlap = np.intersect1d(train_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                overlap = np.intersect1d(train_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                f.write('-----\\n')\n",
    "            \n",
    "            if (end_date_train - (2 * gap_size) - batch_size == datetime.datetime(2019, 12, 1)) or (end_date_train - (2 * gap_size) - batch_size == datetime.datetime(2020, 1, 1)):\n",
    "                network_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                            start_date=datetime.datetime(2019, 11, 1) - batch_size, end_date=datetime.datetime(2019, 11, 1))\n",
    "                network_imos = pd.unique(network_sample[\"IMO number\"])\n",
    "                f.write(f\"start date network sample: {datetime.datetime(2019, 11, 1) - batch_size} and end date network sample: {datetime.datetime(2019, 11, 1)}\\n\")\n",
    "                f.write(f'there are {len(network_imos)} imos sampled for network \\n')\n",
    "                \n",
    "                network_batch = get_subset_include_ids(df=df_classify, include_ids=network_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=datetime.datetime(2019, 11, 1) - encoding_size, end_date=datetime.datetime(2019, 11, 1))\n",
    "\n",
    "                network_imos_temp = pd.unique(network_batch[\"IMO number\"])\n",
    "                f.write(f\"start date network: {datetime.datetime(2019, 11, 1) - encoding_size} and end date network: {datetime.datetime(2019, 11, 1)}\\n\")\n",
    "                f.write(f'there are {len(network_imos_temp)} imos in network and {len(network_batch)} portcalls\\n')\n",
    "\n",
    "                overlap = np.intersect1d(network_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "                overlap = np.intersect1d(network_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "                f.write('-----\\n')\n",
    "            else:\n",
    "                network_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                            start_date=end_date_train - (2 * gap_size) - (2*batch_size), end_date=end_date_train - (2 * gap_size) - batch_size)\n",
    "                network_imos = pd.unique(network_sample[\"IMO number\"])\n",
    "                f.write(f\"start date network sample: {end_date_train - (2 * gap_size) - (2*batch_size)} and end date network sample: {end_date_train - (2 * gap_size) - batch_size}\\n\")\n",
    "                f.write(f'there are {len(network_imos)} imos sampled for network \\n')\n",
    "                \n",
    "                network_batch = get_subset_include_ids(df=df_classify, include_ids=network_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=end_date_train - (2 * gap_size) - batch_size - encoding_size, end_date=end_date_train - (2 * gap_size) - batch_size)\n",
    "\n",
    "                network_imos_temp = pd.unique(network_batch[\"IMO number\"])\n",
    "                f.write(f\"start date network: {end_date_train - (2 * gap_size) - batch_size - encoding_size} and end date network: {end_date_train - (2 * gap_size) - batch_size}\\n\")\n",
    "                f.write(f'there are {len(network_imos_temp)} imos in network and {len(network_batch)} portcalls\\n')\n",
    "\n",
    "                overlap = np.intersect1d(network_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "                overlap = np.intersect1d(network_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "                f.write('-----\\n')\n",
    "\n",
    "            G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(network_batch, id_column=\"IMO number\")\n",
    "            feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "            df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_12 = get_feature_df(test_batch_12, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "\n",
    "            fold_directory = iteration_directory + f'/train_batch_{x+1}/'\n",
    "            if not os.path.isdir(fold_directory):\n",
    "                os.mkdir(fold_directory)\n",
    "\n",
    "            df_features_train = df_features_train.copy()\n",
    "            test_12 = df_features_test_12.copy()\n",
    "\n",
    "            imo_train = df_features_train['IMO'].to_numpy()\n",
    "            imo_test_12 = test_12['IMO'].to_numpy()\n",
    "\n",
    "\n",
    "            np.save(f'{fold_directory}imo_test.npy', np.array(imo_test_12))\n",
    "            np.save(f'{fold_directory}imo_train.npy', np.array(imo_train))\n",
    "        \n",
    "            for target in CLASSES_TO_USE:\n",
    "                name = target.replace('/', '-')\n",
    "                class_directory = fold_directory + f'/class_{name}/'\n",
    "                if not os.path.isdir(class_directory):\n",
    "                    os.mkdir(class_directory)\n",
    "\n",
    "                df = df_features_train.copy()\n",
    "                df_test_12 = test_12.copy()\n",
    "                \n",
    "                #   transform every irrelevant classname to 'other'\n",
    "                df.loc[df['Target'] != target, 'Target'] = 'Other'\n",
    "                X_train = df.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_12.loc[df_test_12['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_12 = df_test_12.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                #   Retrieve a list of unique labels (both string and numeric)\n",
    "                labels = np.array([ 'Other', target])\n",
    "\n",
    "                #   Retrieve numeric labels train\n",
    "                ship_labels = df['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_train = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 12\n",
    "                ship_labels = df_test_12['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_12 = stratify\n",
    "\n",
    "\n",
    "                bst = XGBClassifier(n_estimators=100, max_depth=3, objective='binary:logistic', tree_method='gpu_hist')\n",
    "                np.save(f'{class_directory}y_train.npy', np.array(y_train))\n",
    "\n",
    "                # fit model\n",
    "                bst.fit(X_train, y_train)\n",
    "                bst.save_model(f'{class_directory}model.json')\n",
    "\n",
    "                test_directory = class_directory + f'/test_12/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_12)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_12))\n",
    "            \n",
    "\n",
    "        end_network = time.time()\n",
    "        dur_network = end_network - start_network\n",
    "        f.write(f\"In total from network creation until features took: {dur_network}, which is {dur_network/60} minutes\\n\")\n",
    "\n",
    "        f.close()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPERIMENT Resampled Gap\n",
    "def resampled_gap(df_portcalls, results_directory):\n",
    "    # create necessary files for logging\n",
    "    current_results_directory = results_directory + \"experiment_resampled_gap/\"\n",
    "    if not os.path.isdir(current_results_directory):\n",
    "        os.mkdir(current_results_directory)\n",
    "\n",
    "\n",
    "    # Retrieve the desired timeframe\n",
    "    df = get_time_frame_data(df_portcalls, START_DATE, END_DATE, \"ATA_LT\")\n",
    "\n",
    "\n",
    "    #   only keep relevant columns and set the id column\n",
    "    relevant_columns = {\"ATA_LT\": \"datetime64[ns]\", \n",
    "                    \"ATD_LT\": \"datetime64[ns]\",\n",
    "                        \"Port Name\": \"string\",\n",
    "                        \"IMO Number\": \"string\",\n",
    "                        \"(ATA) Ship Type Description\": \"string\"\n",
    "                        }\n",
    "    new_column_names = [\"Arrival Time\", \"Departure Time\", \"Port Name\", \"IMO number\", \"Ship Type\"]\n",
    "\n",
    "    id_column= \"IMO number\"\n",
    "\n",
    "    df = select_columns(df,relevant_columns=relevant_columns, new_column_names=new_column_names)\n",
    "\n",
    "    #   only keep the classes we want to classify\n",
    "    df = keep_classes(df, classes_to_keep=CLASSES_TO_USE, target_column='Ship Type', id_column='IMO number')\n",
    "   \n",
    "   \n",
    "\n",
    "    #   remove single occurences from network set\n",
    "    df_classify = remove_sinlge_occurences(df, column_name=\"IMO number\")\n",
    "\n",
    "\n",
    "    batch_size = relativedelta(months=1)\n",
    "    encoding_size = relativedelta(months=12)\n",
    "\n",
    "    for run in range(30):\n",
    "        start_network = time.time()\n",
    "\n",
    "        iteration_directory = current_results_directory + f'/run_{run+1}/'\n",
    "        if not os.path.isdir(iteration_directory):\n",
    "            os.mkdir(iteration_directory)\n",
    "\n",
    "        current_run_log = iteration_directory + 'experiment_log.txt'\n",
    "\n",
    "        f = open(current_run_log, 'a')\n",
    "\n",
    "\n",
    "        end_date_train = datetime.datetime(2018, 4, 1) + (run * relativedelta(months=1))\n",
    "        start_date_test = datetime.datetime(2018, 4, 1, microsecond=1) + (run * relativedelta(months=1))\n",
    "        \n",
    "        if start_date_test >= datetime.datetime(2019, 11, 1, microsecond=1):\n",
    "            end_date_train = end_date_train + relativedelta(months=3)\n",
    "            start_date_test = start_date_test + relativedelta(months=3)\n",
    "        \n",
    "        if start_date_test + batch_size <= END_DATE:\n",
    "            end_date_test = start_date_test + batch_size\n",
    "        else:\n",
    "            end_date_test = END_DATE\n",
    "\n",
    "        test_sample = get_time_frame_data(df_classify, start_date_test, end_date_test, \"Arrival Time\")\n",
    "        test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.1)\n",
    "        test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "        f.write(f\"start date test sample: {start_date_test} and end date test sample: {end_date_test}\\n\")\n",
    "        f.write(f'Amount of imos in test sample {len(test_imos)}\\n')        \n",
    "\n",
    "        test_batch_12 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=12), end_date=end_date_test)\n",
    "        f.write(f\"start date test 12: {end_date_test - relativedelta(months=12)} and end date test 12: {end_date_test}\\n\")\n",
    "        imos_12 = pd.unique(test_batch_12[\"IMO number\"])\n",
    "\n",
    "        f.write(f'in test 12 there are {len(imos_12)} imos and {len(test_batch_12)} portcalls\\n')\n",
    "\n",
    "\n",
    "\n",
    "        for x in range(40):\n",
    "            f.write(f'train batch {x+1}:\\n')\n",
    "            # account for nov/dec 2019\n",
    "            if (end_date_train - (x * batch_size) == datetime.datetime(2019, 12, 1)) or (end_date_train - (x * batch_size) == datetime.datetime(2020, 1, 1)):\n",
    "                f.write(\"ACCOUNTING FOR THE DATA LOSS IN NOVEMBER/DECEMBER 2019 \\n \")\n",
    "                train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=datetime.datetime(2019, 11, 1) - batch_size , end_date=datetime.datetime(2019, 11, 1))\n",
    "                train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "                f.write(f\"start date train sample: {datetime.datetime(2019, 11, 1) - batch_size} and end date train sample: {datetime.datetime(2019, 11, 1)}\\n\")\n",
    "                f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "                \n",
    "                train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=datetime.datetime(2019, 11, 1) - encoding_size, end_date=datetime.datetime(2019, 11, 1))\n",
    "\n",
    "                train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "                f.write(f\"start date train: {datetime.datetime(2019, 11, 1) - encoding_size} and end date train: {datetime.datetime(2019, 11, 1) - encoding_size}\\n\")\n",
    "                f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "                overlap = np.intersect1d(train_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                overlap = np.intersect1d(train_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                f.write('-----\\n')\n",
    "\n",
    "            else: \n",
    "                train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=end_date_train - ((1+x) * batch_size) , end_date=end_date_train - (x * batch_size))\n",
    "                train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "                f.write(f\"start date train sample: {end_date_train - ((1+x) * batch_size)} and end date train sample: {end_date_train - (x * batch_size)}\\n\")\n",
    "                f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "                \n",
    "                train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                        start_date=end_date_train - (x * batch_size) - encoding_size, end_date=end_date_train - (x * batch_size))\n",
    "\n",
    "                train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "                f.write(f\"start date train: {end_date_train - (x * batch_size) - encoding_size} and end date train: {end_date_train - (x * batch_size)}\\n\")\n",
    "                f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "                overlap = np.intersect1d(train_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                overlap = np.intersect1d(train_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "                f.write('-----\\n')\n",
    "\n",
    "            G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(train_batch, id_column=\"IMO number\")\n",
    "            feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "            df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_12 = get_feature_df(test_batch_12, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "\n",
    "\n",
    "            fold_directory = iteration_directory + f'/train_batch_{x+1}/'\n",
    "            if not os.path.isdir(fold_directory):\n",
    "                os.mkdir(fold_directory)\n",
    "\n",
    "            df_features_train = df_features_train.copy()\n",
    "            test_12 = df_features_test_12.copy()\n",
    "\n",
    "            imo_train = df_features_train['IMO'].to_numpy()\n",
    "            imo_test_12 = test_12['IMO'].to_numpy()\n",
    "\n",
    "            np.save(f'{fold_directory}imo_test.npy', np.array(imo_test_12))\n",
    "            np.save(f'{fold_directory}imo_train.npy', np.array(imo_train))\n",
    "        \n",
    "            for target in CLASSES_TO_USE:\n",
    "                name = target.replace('/', '-')\n",
    "                class_directory = fold_directory + f'/class_{name}/'\n",
    "                if not os.path.isdir(class_directory):\n",
    "                    os.mkdir(class_directory)\n",
    "\n",
    "                df = df_features_train.copy()\n",
    "                df_test_12 = test_12.copy()\n",
    "                \n",
    "                #   transform every irrelevant classname to 'other'\n",
    "                df.loc[df['Target'] != target, 'Target'] = 'Other'\n",
    "                X_train = df.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_12.loc[df_test_12['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_12 = df_test_12.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                #   Retrieve a list of unique labels (both string and numeric)\n",
    "                labels = np.array([ 'Other', target])\n",
    "                numeric_labels = np.array([0,1])\n",
    "\n",
    "                #   Retrieve numeric labels train\n",
    "                ship_labels = df['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_train = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 12\n",
    "                ship_labels = df_test_12['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_12 = stratify\n",
    "\n",
    "\n",
    "\n",
    "                bst = XGBClassifier(n_estimators=100, max_depth=3, objective='binary:logistic', tree_method='gpu_hist')\n",
    "                np.save(f'{class_directory}y_train.npy', np.array(y_train))\n",
    "\n",
    "                # fit model\n",
    "                bst.fit(X_train, y_train)\n",
    "                bst.save_model(f'{class_directory}model.json')\n",
    "\n",
    "                test_directory = class_directory + f'/test_12/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_12)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_12))\n",
    "            \n",
    "\n",
    "        end_network = time.time()\n",
    "        dur_network = end_network - start_network\n",
    "        f.write(f\"In total from network creation until features took: {dur_network}, which is {dur_network/60} minutes\\n\")\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding window (static and resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPERIMENT Static expanding window\n",
    "\n",
    "def static_expanding(df_portcalls, results_directory):\n",
    "    # create necessary files for logging\n",
    "    current_results_directory = results_directory + \"experiment_static_expanding/\"\n",
    "    if not os.path.isdir(current_results_directory):\n",
    "        os.mkdir(current_results_directory)\n",
    "\n",
    "\n",
    "    # Retrieve the desired timeframe\n",
    "    df = get_time_frame_data(df_portcalls, START_DATE, END_DATE, \"ATA_LT\")\n",
    "\n",
    "\n",
    "    #   only keep relevant columns and set the id column\n",
    "    relevant_columns = {\"ATA_LT\": \"datetime64[ns]\", \n",
    "                    \"ATD_LT\": \"datetime64[ns]\",\n",
    "                        \"Port Name\": \"string\",\n",
    "                        \"IMO Number\": \"string\",\n",
    "                        \"(ATA) Ship Type Description\": \"string\"\n",
    "                        }\n",
    "    new_column_names = [\"Arrival Time\", \"Departure Time\", \"Port Name\", \"IMO number\", \"Ship Type\"]\n",
    "\n",
    "    id_column= \"IMO number\"\n",
    "\n",
    "    df = select_columns(df,relevant_columns=relevant_columns, new_column_names=new_column_names)\n",
    "\n",
    "    #   only keep the classes we want to classify\n",
    "    df = keep_classes(df, classes_to_keep=CLASSES_TO_USE, target_column='Ship Type', id_column='IMO number')\n",
    "   \n",
    "    #   remove single occurences from network set\n",
    "    df_classify = remove_sinlge_occurences(df, column_name=\"IMO number\")\n",
    "\n",
    "\n",
    "    batch_size = relativedelta(months=1)\n",
    "    encoding_size = relativedelta(months=12)\n",
    "\n",
    "    for run in range(30):\n",
    "        start_network = time.time()\n",
    "\n",
    "        iteration_directory = current_results_directory + f'/run_{run+1}/'\n",
    "        if not os.path.isdir(iteration_directory):\n",
    "            os.mkdir(iteration_directory)\n",
    "\n",
    "        current_run_log = iteration_directory + 'experiment_log.txt'\n",
    "\n",
    "        f = open(current_run_log, 'a')\n",
    "\n",
    "\n",
    "        end_date_train = datetime.datetime(2018, 4, 1) + (run * relativedelta(months=1))\n",
    "        start_date_test = datetime.datetime(2018, 4, 1, microsecond=1) + (run * relativedelta(months=1))\n",
    "        \n",
    "        # account for data scarsity in two months\n",
    "        if start_date_test >= datetime.datetime(2019, 11, 1, microsecond=1):\n",
    "            end_date_train = end_date_train + relativedelta(months=3)\n",
    "            start_date_test = start_date_test + relativedelta(months=3)\n",
    "        \n",
    "        if start_date_test + batch_size <= END_DATE:\n",
    "            end_date_test = start_date_test + batch_size\n",
    "        else:\n",
    "            end_date_test = END_DATE\n",
    "\n",
    "        test_sample = get_time_frame_data(df_classify, start_date_test, end_date_test, \"Arrival Time\")\n",
    "        test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.1)\n",
    "        test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "        f.write(f\"start date test sample: {start_date_test} and end date test sample: {end_date_test}\\n\")\n",
    "        f.write(f'Amount of imos in test sample {len(test_imos)}\\n')        \n",
    "\n",
    "        test_batch_12 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=12), end_date=end_date_test)\n",
    "        f.write(f\"start date test 12: {end_date_test - relativedelta(months=12)} and end date test 12: {end_date_test}\\n\")\n",
    "        imos_12 = pd.unique(test_batch_12[\"IMO number\"])\n",
    "\n",
    "        test_batch_6 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=6), end_date=end_date_test)\n",
    "        f.write(f\"start date test 6: {end_date_test - relativedelta(months=6)} and end date test 6: {end_date_test}\\n\")\n",
    "        imos_6 = pd.unique(test_batch_6[\"IMO number\"])\n",
    "\n",
    "        test_batch_3 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=3), end_date=end_date_test)\n",
    "        f.write(f\"start date test 3: {end_date_test - relativedelta(months=3)} and end date test 3: {end_date_test}\\n\")\n",
    "        imos_3 = pd.unique(test_batch_3[\"IMO number\"])\n",
    "\n",
    "        test_batch_1 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=1), end_date=end_date_test)\n",
    "        f.write(f\"start date test 1: {end_date_test - relativedelta(months=1)} and end date test 1: {end_date_test}\\n\")\n",
    "        imos_1 = pd.unique(test_batch_1[\"IMO number\"])\n",
    "\n",
    "        f.write(f'in test 12 there are {len(imos_12)} imos and {len(test_batch_12)} portcalls\\n')\n",
    "        f.write(f'in test 6 there are {len(imos_6)} imos and {len(test_batch_6)} portcalls\\n')\n",
    "        f.write(f'in test 3 there are {len(imos_3)} imos and {len(test_batch_3)} portcalls\\n')\n",
    "        f.write(f'in test 1 there are {len(imos_1)} imos and {len(test_batch_1)} portcalls\\n')\n",
    "\n",
    "        for x in range(40):\n",
    "            f.write(f'train batch {x+1}:\\n')\n",
    "\n",
    "            train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_train - ((1+x) * batch_size) , end_date=end_date_train)\n",
    "            train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "            f.write(f\"start date train sample: {end_date_train - ((1+x) * batch_size)} and end date train sample: {end_date_train}\\n\")\n",
    "            f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "            \n",
    "            train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_train - (x * batch_size) - encoding_size, end_date=end_date_train)\n",
    "\n",
    "            train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "            f.write(f\"start date train: {end_date_train - (x * batch_size) - encoding_size} and end date train: {end_date_train}\\n\")\n",
    "            f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "            overlap = np.intersect1d(train_imos, test_imos)\n",
    "            f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "            overlap = np.intersect1d(train_imos_temp, test_imos)\n",
    "            f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "            f.write('\\n')\n",
    "\n",
    "            # account for data scarsity in two months\n",
    "            if (end_date_train - (x * batch_size) == datetime.datetime(2019, 12, 1)) or (end_date_train - (x * batch_size) == datetime.datetime(2020, 1, 1)):\n",
    "                network_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                    start_date=datetime.datetime(2020, 2, 1) - batch_size, end_date=datetime.datetime(2020, 2, 1))\n",
    "                network_imos = pd.unique(network_sample[\"IMO number\"])\n",
    "                f.write(f\"start date network sample: {datetime.datetime(2020, 2, 1) - batch_size} and end network sample: {datetime.datetime(2020, 2, 1)}\\n\")\n",
    "                f.write(f'there are {len(network_imos)} imos sampled for network \\n')\n",
    "\n",
    "                network_batch = get_subset_include_ids(df=df_classify, include_ids=network_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                    start_date=datetime.datetime(2020, 2, 1) - encoding_size, end_date=datetime.datetime(2020, 2, 1))\n",
    "\n",
    "                network_imos_temp = pd.unique(network_batch[\"IMO number\"])\n",
    "\n",
    "                overlap = np.intersect1d(network_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "                overlap = np.intersect1d(network_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "\n",
    "                overlap = np.intersect1d(network_imos, train_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and train\\n')\n",
    "                overlap = np.intersect1d(network_imos_temp, train_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and train\\n')\n",
    "\n",
    "            else: \n",
    "                network_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                    start_date=end_date_train - ((1+x) * batch_size) , end_date=end_date_train - (x * batch_size))\n",
    "                network_imos = pd.unique(network_sample[\"IMO number\"])\n",
    "                f.write(f\"start date network sample: {end_date_train - ((1+x) * batch_size)} and end network sample: {end_date_train - (x * batch_size)}\\n\")\n",
    "                f.write(f'there are {len(network_imos)} imos sampled for network \\n')\n",
    "\n",
    "                network_batch = get_subset_include_ids(df=df_classify, include_ids=network_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                    start_date=end_date_train - (x * batch_size) - encoding_size, end_date=end_date_train - (x * batch_size))\n",
    "\n",
    "                network_imos_temp = pd.unique(network_batch[\"IMO number\"])\n",
    "\n",
    "                overlap = np.intersect1d(network_imos, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "                overlap = np.intersect1d(network_imos_temp, test_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and test\\n')\n",
    "\n",
    "                overlap = np.intersect1d(network_imos, train_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and train\\n')\n",
    "                overlap = np.intersect1d(network_imos_temp, train_imos)\n",
    "                f.write(f'there are {len(overlap)} imos overlapping between network and train\\n')\n",
    "\n",
    "            G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(network_batch, id_column=\"IMO number\")\n",
    "            feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "            df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_12 = get_feature_df(test_batch_12, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_6 = get_feature_df(test_batch_6, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_3 = get_feature_df(test_batch_3, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')     \n",
    "            df_features_test_1 = get_feature_df(test_batch_1, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "\n",
    "            fold_directory = iteration_directory + f'/train_batch_{x+1}/'\n",
    "            if not os.path.isdir(fold_directory):\n",
    "                os.mkdir(fold_directory)\n",
    "\n",
    "            df_features_train = df_features_train.copy()\n",
    "            test_12 = df_features_test_12.copy()\n",
    "            test_6 = df_features_test_6.copy()\n",
    "            test_3 = df_features_test_3.copy()\n",
    "            test_1 = df_features_test_1.copy()\n",
    "\n",
    "            imo_train = df_features_train['IMO'].to_numpy()\n",
    "            imo_test_12 = test_12['IMO'].to_numpy()\n",
    "            imo_test_6 = test_6['IMO'].to_numpy()\n",
    "            imo_test_3 = test_3['IMO'].to_numpy()\n",
    "            imo_test_1 = test_1['IMO'].to_numpy()\n",
    "\n",
    "            np.save(f'{fold_directory}imo_test_12.npy', np.array(imo_test_12))\n",
    "            np.save(f'{fold_directory}imo_test_6.npy', np.array(imo_test_6))\n",
    "            np.save(f'{fold_directory}imo_test_3.npy', np.array(imo_test_3))\n",
    "            np.save(f'{fold_directory}imo_test_1.npy', np.array(imo_test_1))\n",
    "            np.save(f'{fold_directory}imo_train.npy', np.array(imo_train))\n",
    "        \n",
    "            for target in CLASSES_TO_USE:\n",
    "                name = target.replace('/', '-')\n",
    "                class_directory = fold_directory + f'/class_{name}/'\n",
    "                if not os.path.isdir(class_directory):\n",
    "                    os.mkdir(class_directory)\n",
    "\n",
    "                df = df_features_train.copy()\n",
    "                df_test_12 = test_12.copy()\n",
    "                df_test_6 = test_6.copy()\n",
    "                df_test_3 = test_3.copy()\n",
    "                df_test_1 = test_1.copy()\n",
    "                \n",
    "                #   transform every irrelevant classname to 'other'\n",
    "                df.loc[df['Target'] != target, 'Target'] = 'Other'\n",
    "                X_train = df.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_12.loc[df_test_12['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_12 = df_test_12.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_6.loc[df_test_6['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_6 = df_test_6.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_3.loc[df_test_3['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_3 = df_test_3.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_1.loc[df_test_1['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_1 = df_test_1.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                #   Retrieve a list of unique labels (both string and numeric)\n",
    "                labels = np.array([ 'Other', target])\n",
    "                numeric_labels = np.array([0,1])\n",
    "\n",
    "                #   Retrieve numeric labels train\n",
    "                ship_labels = df['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_train = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 12\n",
    "                ship_labels = df_test_12['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_12 = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 6\n",
    "                ship_labels = df_test_6['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_6 = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 3\n",
    "                ship_labels = df_test_3['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_3 = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 1\n",
    "                ship_labels = df_test_1['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_1 = stratify\n",
    "\n",
    "\n",
    "                bst = XGBClassifier(n_estimators=100, max_depth=3, objective='binary:logistic', tree_method='gpu_hist')\n",
    "                np.save(f'{class_directory}y_train.npy', np.array(y_train))\n",
    "\n",
    "                # fit model\n",
    "                bst.fit(X_train, y_train)\n",
    "                bst.save_model(f'{class_directory}model.json')\n",
    "\n",
    "                test_directory = class_directory + f'/test_12/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_12)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_12))\n",
    "\n",
    "\n",
    "                test_directory = class_directory + f'/test_6/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_6)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_6))\n",
    "\n",
    "\n",
    "                test_directory = class_directory + f'/test_3/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_3)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_3))\n",
    "\n",
    "                test_directory = class_directory + f'/test_1/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_1)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_1))\n",
    "            f.write('---------------------------------------\\n')\n",
    "            \n",
    "\n",
    "\n",
    "        end_network = time.time()\n",
    "        dur_network = end_network - start_network\n",
    "        f.write(f\"In total from network creation until features took: {dur_network}, which is {dur_network/60} minutes\\n\")\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPERIMENT Resampled expanding window\n",
    "def resampled_expanding(df_portcalls, results_directory):\n",
    "    # create necessary files for logging\n",
    "    current_results_directory = results_directory + \"experiment_resampled_expanding/\"\n",
    "    if not os.path.isdir(current_results_directory):\n",
    "        os.mkdir(current_results_directory)\n",
    "\n",
    "\n",
    "    # Retrieve the desired timeframe\n",
    "    df = get_time_frame_data(df_portcalls, START_DATE, END_DATE, \"ATA_LT\")\n",
    "\n",
    "\n",
    "    #   only keep relevant columns and set the id column\n",
    "    relevant_columns = {\"ATA_LT\": \"datetime64[ns]\", \n",
    "                    \"ATD_LT\": \"datetime64[ns]\",\n",
    "                        \"Port Name\": \"string\",\n",
    "                        \"IMO Number\": \"string\",\n",
    "                        \"(ATA) Ship Type Description\": \"string\"\n",
    "                        }\n",
    "    new_column_names = [\"Arrival Time\", \"Departure Time\", \"Port Name\", \"IMO number\", \"Ship Type\"]\n",
    "\n",
    "    id_column= \"IMO number\"\n",
    "\n",
    "    df = select_columns(df,relevant_columns=relevant_columns, new_column_names=new_column_names)\n",
    "\n",
    "    #   only keep the classes we want to classify\n",
    "    df = keep_classes(df, classes_to_keep=CLASSES_TO_USE, target_column='Ship Type', id_column='IMO number')\n",
    "   \n",
    "   \n",
    "\n",
    "    #   remove single occurences from network set\n",
    "    df_classify = remove_sinlge_occurences(df, column_name=\"IMO number\")\n",
    "\n",
    "\n",
    "    batch_size = relativedelta(months=1)\n",
    "    encoding_size = relativedelta(months=12)\n",
    "\n",
    "    for run in range(30):\n",
    "        start_network = time.time()\n",
    "\n",
    "        iteration_directory = current_results_directory + f'/run_{run+1}/'\n",
    "        if not os.path.isdir(iteration_directory):\n",
    "            os.mkdir(iteration_directory)\n",
    "\n",
    "        current_run_log = iteration_directory + 'experiment_log.txt'\n",
    "\n",
    "        f = open(current_run_log, 'a')\n",
    "\n",
    "\n",
    "        end_date_train = datetime.datetime(2018, 4, 1) + (run * relativedelta(months=1))\n",
    "        start_date_test = datetime.datetime(2018, 4, 1, microsecond=1) + (run * relativedelta(months=1))\n",
    "\n",
    "\n",
    "        # account for data scarsity in two months\n",
    "        if start_date_test >= datetime.datetime(2019, 11, 1, microsecond=1):\n",
    "            end_date_train = end_date_train + relativedelta(months=3)\n",
    "            start_date_test = start_date_test + relativedelta(months=3)\n",
    "        \n",
    "        if start_date_test + batch_size <= END_DATE:\n",
    "            end_date_test = start_date_test + batch_size\n",
    "        else:\n",
    "            end_date_test = END_DATE\n",
    "\n",
    "        test_sample = get_time_frame_data(df_classify, start_date_test, end_date_test, \"Arrival Time\")\n",
    "        test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.1)\n",
    "        test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "        f.write(f\"start date test sample: {start_date_test} and end date test sample: {end_date_test}\\n\")\n",
    "        f.write(f'Amount of imos in test sample {len(test_imos)}\\n')        \n",
    "\n",
    "        test_batch_12 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=12), end_date=end_date_test)\n",
    "        f.write(f\"start date test 12: {end_date_test - relativedelta(months=12)} and end date test 12: {end_date_test}\\n\")\n",
    "        imos_12 = pd.unique(test_batch_12[\"IMO number\"])\n",
    "\n",
    "        test_batch_6 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=6), end_date=end_date_test)\n",
    "        f.write(f\"start date test 6: {end_date_test - relativedelta(months=6)} and end date test 6: {end_date_test}\\n\")\n",
    "        imos_6 = pd.unique(test_batch_6[\"IMO number\"])\n",
    "\n",
    "        test_batch_3 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=3), end_date=end_date_test)\n",
    "        f.write(f\"start date test 3: {end_date_test - relativedelta(months=3)} and end date test 3: {end_date_test}\\n\")\n",
    "        imos_3 = pd.unique(test_batch_3[\"IMO number\"])\n",
    "\n",
    "        test_batch_1 = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_test - relativedelta(months=1), end_date=end_date_test)\n",
    "        f.write(f\"start date test 1: {end_date_test - relativedelta(months=1)} and end date test 1: {end_date_test}\\n\")\n",
    "        imos_1 = pd.unique(test_batch_1[\"IMO number\"])\n",
    "\n",
    "        f.write(f'in test 12 there are {len(imos_12)} imos and {len(test_batch_12)} portcalls\\n')\n",
    "        f.write(f'in test 6 there are {len(imos_6)} imos and {len(test_batch_6)} portcalls\\n')\n",
    "        f.write(f'in test 3 there are {len(imos_3)} imos and {len(test_batch_3)} portcalls\\n')\n",
    "        f.write(f'in test 1 there are {len(imos_1)} imos and {len(test_batch_1)} portcalls\\n')\n",
    "\n",
    "\n",
    "        for x in range(40):\n",
    "            f.write(f'train batch {x+1}:\\n')\n",
    "\n",
    "            train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_train - ((1+x) * batch_size) , end_date=end_date_train)\n",
    "            train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "            f.write(f\"start date train sample: {end_date_train - ((1+x) * batch_size)} and end date train sample: {end_date_train}\\n\")\n",
    "            f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "            \n",
    "            train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                  start_date=end_date_train - (x * batch_size) - encoding_size, end_date=end_date_train)\n",
    "\n",
    "            train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "            f.write(f\"start date train: {end_date_train - (x * batch_size) - encoding_size} and end date train: {end_date_train}\\n\")\n",
    "            f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "            overlap = np.intersect1d(train_imos, test_imos)\n",
    "            f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "            overlap = np.intersect1d(train_imos_temp, test_imos)\n",
    "            f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "            \n",
    "\n",
    "            G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(train_batch, id_column=\"IMO number\")\n",
    "            feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "            df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_12 = get_feature_df(test_batch_12, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_6 = get_feature_df(test_batch_6, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_3 = get_feature_df(test_batch_3, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "            df_features_test_1 = get_feature_df(test_batch_1, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "\n",
    "\n",
    "            fold_directory = iteration_directory + f'/train_batch_{x+1}/'\n",
    "            if not os.path.isdir(fold_directory):\n",
    "                os.mkdir(fold_directory)\n",
    "\n",
    "            df_features_train = df_features_train.copy()\n",
    "            test_12 = df_features_test_12.copy()\n",
    "            test_6 = df_features_test_6.copy()\n",
    "            test_3 = df_features_test_3.copy()\n",
    "            test_1 = df_features_test_1.copy()\n",
    "\n",
    "            imo_train = df_features_train['IMO'].to_numpy()\n",
    "            imo_test_12 = test_12['IMO'].to_numpy()\n",
    "            imo_test_6 = test_6['IMO'].to_numpy()\n",
    "            imo_test_3 = test_3['IMO'].to_numpy()\n",
    "            imo_test_1 = test_1['IMO'].to_numpy()\n",
    "\n",
    "            np.save(f'{fold_directory}imo_test_12.npy', np.array(imo_test_12))\n",
    "            np.save(f'{fold_directory}imo_test_6.npy', np.array(imo_test_6))\n",
    "            np.save(f'{fold_directory}imo_test_3.npy', np.array(imo_test_3))\n",
    "            np.save(f'{fold_directory}imo_test_1.npy', np.array(imo_test_1))\n",
    "            np.save(f'{fold_directory}imo_train.npy', np.array(imo_train))\n",
    "        \n",
    "            for target in CLASSES_TO_USE:\n",
    "                name = target.replace('/', '-')\n",
    "                class_directory = fold_directory + f'/class_{name}/'\n",
    "                if not os.path.isdir(class_directory):\n",
    "                    os.mkdir(class_directory)\n",
    "\n",
    "                df = df_features_train.copy()\n",
    "                df_test_12 = test_12.copy()\n",
    "                df_test_6 = test_6.copy()\n",
    "                df_test_3 = test_3.copy()\n",
    "                df_test_1 = test_1.copy()\n",
    "                \n",
    "                #   transform every irrelevant classname to 'other'\n",
    "                df.loc[df['Target'] != target, 'Target'] = 'Other'\n",
    "                X_train = df.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_12.loc[df_test_12['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_12 = df_test_12.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_6.loc[df_test_6['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_6 = df_test_6.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_3.loc[df_test_3['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_3 = df_test_3.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                df_test_1.loc[df_test_1['Target'] != target, 'Target'] = 'Other'\n",
    "                X_test_1 = df_test_1.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "                #   Retrieve a list of unique labels (both string and numeric)\n",
    "                labels = np.array([ 'Other', target])\n",
    "                numeric_labels = np.array([0,1])\n",
    "\n",
    "                #   Retrieve numeric labels train\n",
    "                ship_labels = df['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_train = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 12\n",
    "                ship_labels = df_test_12['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_12 = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 6\n",
    "                ship_labels = df_test_6['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_6 = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 3\n",
    "                ship_labels = df_test_3['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_3 = stratify\n",
    "\n",
    "                #   Retrieve numeric labels test 1\n",
    "                ship_labels = df_test_1['Target'].to_numpy()\n",
    "                stratify = []\n",
    "\n",
    "                for ship in ship_labels:\n",
    "                    index = np.where(labels == ship)\n",
    "                    stratify.append(index[0][0])\n",
    "\n",
    "                stratify = np.array(stratify)\n",
    "\n",
    "                y_test_1 = stratify\n",
    "\n",
    "\n",
    "                bst = XGBClassifier(n_estimators=100, max_depth=3, objective='binary:logistic', tree_method='gpu_hist')\n",
    "                np.save(f'{class_directory}y_train.npy', np.array(y_train))\n",
    "\n",
    "                # fit model\n",
    "                bst.fit(X_train, y_train)\n",
    "                bst.save_model(f'{class_directory}model.json')\n",
    "\n",
    "                test_directory = class_directory + f'/test_12/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_12)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_12))\n",
    "\n",
    "\n",
    "                test_directory = class_directory + f'/test_6/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_6)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_6))\n",
    "\n",
    "\n",
    "                test_directory = class_directory + f'/test_3/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_3)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_3))\n",
    "\n",
    "                test_directory = class_directory + f'/test_1/'\n",
    "                if not os.path.isdir(test_directory):\n",
    "                    os.mkdir(test_directory)\n",
    "                \n",
    "                # both make predicition in probabilities and normal binary\n",
    "                y_pred_prob = bst.predict_proba(X_test_1)\n",
    "                y_pred_prob = np.array(y_pred_prob)\n",
    "                y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "                y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "                np.save(f'{test_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "                np.save(f'{test_directory}y_pred.npy', np.array(y_pred))\n",
    "                np.save(f'{test_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "                np.save(f'{test_directory}y_test.npy', np.array(y_test_1))\n",
    "            \n",
    "            f.write('-------------\\n')\n",
    "\n",
    "\n",
    "        end_network = time.time()\n",
    "        dur_network = end_network - start_network\n",
    "        f.write(f\"In total from network creation until features took: {dur_network}, which is {dur_network/60} minutes\\n\")\n",
    "\n",
    "        f.close()\n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding window (static and resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPERIMENT Static sliding window\n",
    "\n",
    "def static_sliding(df_portcalls, results_directory):\n",
    "    # create necessary files for logging\n",
    "    current_results_directory = results_directory + \"experiment_static_sliding/\"\n",
    "    if not os.path.isdir(current_results_directory):\n",
    "        os.mkdir(current_results_directory)\n",
    "\n",
    "\n",
    "    # Retrieve the desired timeframe\n",
    "    df = get_time_frame_data(df_portcalls, START_DATE, END_DATE, \"ATA_LT\")\n",
    "\n",
    "\n",
    "    #   only keep relevant columns and set the id column\n",
    "    relevant_columns = {\"ATA_LT\": \"datetime64[ns]\", \n",
    "                        \"ATD_LT\": \"datetime64[ns]\",\n",
    "                        \"Port Name\": \"string\",\n",
    "                        \"IMO Number\": \"string\",\n",
    "                        \"(ATA) Ship Type Description\": \"string\"\n",
    "                        }\n",
    "    new_column_names = [\"Arrival Time\", \"Departure Time\", \"Port Name\", \"IMO number\", \"Ship Type\"]\n",
    "\n",
    "    id_column= \"IMO number\"\n",
    "\n",
    "    df = select_columns(df,relevant_columns=relevant_columns, new_column_names=new_column_names)\n",
    "\n",
    "    #   only keep the classes we want to classify\n",
    "    df = keep_classes(df, classes_to_keep=CLASSES_TO_USE, target_column='Ship Type', id_column='IMO number')\n",
    "   \n",
    "   \n",
    "\n",
    "    #   remove single occurences from network set\n",
    "    df_classify = remove_sinlge_occurences(df, column_name=\"IMO number\")\n",
    "\n",
    "\n",
    "    batch_size = relativedelta(months=1)\n",
    "    encoding_size = relativedelta(months=12)\n",
    "\n",
    "    \n",
    "    start_network = time.time()\n",
    "\n",
    "    current_run_log = current_results_directory + 'experiment_log.txt'\n",
    "\n",
    "    f = open(current_run_log, 'a')\n",
    "\n",
    "    df_train_batches = []\n",
    "    df_test_batches = []\n",
    "\n",
    "    end_date_train = datetime.datetime(2017, 6, 1)\n",
    "    end_date_test = datetime.datetime(2017, 7, 1, microsecond=1) \n",
    "\n",
    "    f.write(f'train batch 1:\\n')\n",
    "\n",
    "    test_sample = get_time_frame_data(df_classify, end_date_test- batch_size, end_date_test, \"Arrival Time\")\n",
    "    \n",
    "    test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.1)\n",
    "    \n",
    "    test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "    f.write(f\"start date test sample: {end_date_test - batch_size} and end date test sample: {end_date_test}\\n\")\n",
    "    f.write(f'Amount of imos in test sample {len(test_imos)}\\n')        \n",
    "\n",
    "    test_batch = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_test - encoding_size, end_date=end_date_test)\n",
    "    f.write(f\"start date test: {end_date_test - encoding_size} and end date test: {end_date_test}\\n\")\n",
    "    test_imos = pd.unique(test_batch[\"IMO number\"])\n",
    "\n",
    "    train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                            start_date=end_date_train - batch_size, end_date=end_date_train)\n",
    "\n",
    "    \n",
    "    train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "    f.write(f\"start date train sample: {end_date_train - batch_size} and end date train sample: {end_date_train}\\n\")\n",
    "    f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "    \n",
    "    train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                            start_date=end_date_train - encoding_size, end_date=end_date_train)\n",
    "\n",
    "    train_imos = pd.unique(train_batch[\"IMO number\"])\n",
    "    network_imos = pd.unique(train_batch[\"IMO number\"])\n",
    "    f.write(f\"start date train: {end_date_train - encoding_size} and end date train: {end_date_train}\\n\")\n",
    "    f.write(f'there are {len(train_imos)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "    \n",
    "\n",
    "    overlap = np.intersect1d(train_imos, test_imos)\n",
    "    f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "    f.write('-----\\n')\n",
    "\n",
    "\n",
    "    G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(train_batch, id_column=\"IMO number\")\n",
    "    feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "    df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "    df_train_batches.append(df_features_train)\n",
    "\n",
    "    df_features_test = get_feature_df(test_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "    df_test_batches.append(df_features_test)\n",
    "\n",
    "    for x in range(40):\n",
    "        f.write(f'train batch {x+1}:\\n')\n",
    "        end_date_test += batch_size\n",
    "        end_date_train += batch_size \n",
    "        if end_date_test == datetime.datetime(2019, 12, 1, microsecond=1):\n",
    "            end_date_train = end_date_train + relativedelta(months=3)\n",
    "            end_date_test = end_date_test + relativedelta(months=3)\n",
    "        \n",
    "        if end_date_test >= END_DATE:\n",
    "            end_date_test = END_DATE\n",
    "\n",
    "        test_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=network_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_test - batch_size, end_date=end_date_test)\n",
    "        test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.3)\n",
    "        test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "        f.write(f\"start date test sample: {end_date_test - batch_size} and end date test sample: {end_date_test}\\n\")\n",
    "        f.write(f'there are {len(test_imos)} imos sampled for test \\n')\n",
    "        \n",
    "        test_batch = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_test - encoding_size, end_date=end_date_test)\n",
    "\n",
    "        test_imos_temp = pd.unique(test_batch[\"IMO number\"])\n",
    "        f.write(f\"start date test: {end_date_test - encoding_size} and end date test: {end_date_test}\\n\")\n",
    "        f.write(f'there are {len(test_imos_temp)} imos in test and {len(test_batch)} portcalls\\n')\n",
    "\n",
    "        train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_train - batch_size, end_date=end_date_train)\n",
    "        \n",
    "        train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "        f.write(f\"start date train sample: {end_date_train - batch_size} and end date train sample: {end_date_train}\\n\")\n",
    "        f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "        \n",
    "        train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_train - encoding_size, end_date=end_date_train)\n",
    "\n",
    "        train_imos_temp = pd.unique(train_batch[\"IMO number\"])\n",
    "        f.write(f\"start date train: {end_date_train - encoding_size} and end date train: {end_date_train}\\n\")\n",
    "        f.write(f'there are {len(train_imos_temp)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "\n",
    "        overlap = np.intersect1d(train_imos, test_imos)\n",
    "        f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "        overlap = np.intersect1d(train_imos_temp, test_imos_temp)\n",
    "        f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "        overlap_network = np.intersect1d(network_imos, test_imos_temp)\n",
    "        f.write(f'there are {len(overlap_network)} imos overlapping between network and test \\n')\n",
    "        f.write('-----\\n')\n",
    "\n",
    "        \n",
    "\n",
    "        df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "        df_train_batches.append(df_features_train)\n",
    "\n",
    "        df_features_test = get_feature_df(test_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "        df_test_batches.append(df_features_test)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    end_network = time.time()\n",
    "    dur_network = end_network - start_network\n",
    "    f.write(f\"In total from network creation until features took: {dur_network}, which is {dur_network/60} minutes\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Do the training and reporting\n",
    "    for i in range(len(df_train_batches)):\n",
    "        fold_directory = current_results_directory + f'/train_batch_{i+1}/'\n",
    "        if not os.path.isdir(fold_directory):\n",
    "            os.mkdir(fold_directory)\n",
    "\n",
    "        df_features_train = df_train_batches[i].copy()\n",
    "        test = df_test_batches[i].copy()\n",
    "\n",
    "        imo_train = df_features_train['IMO'].to_numpy()\n",
    "        imo_test = test['IMO'].to_numpy()\n",
    "\n",
    "        np.save(f'{fold_directory}imo_test.npy', np.array(imo_test))\n",
    "        np.save(f'{fold_directory}imo_train.npy', np.array(imo_train))\n",
    "    \n",
    "        for target in CLASSES_TO_USE:\n",
    "            name = target.replace('/', '-')\n",
    "            class_directory = fold_directory + f'/class_{name}/'\n",
    "            if not os.path.isdir(class_directory):\n",
    "                os.mkdir(class_directory)\n",
    "\n",
    "            df = df_features_train.copy()\n",
    "            df_test = test.copy()\n",
    "    \n",
    "            \n",
    "            #   transform every irrelevant classname to 'other'\n",
    "            df.loc[df['Target'] != target, 'Target'] = 'Other'\n",
    "            X_train = df.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "            df_test.loc[df_test['Target'] != target, 'Target'] = 'Other'\n",
    "            X_test = df_test.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "\n",
    "            #   Retrieve a list of unique labels (both string and numeric)\n",
    "            labels = np.array([ 'Other', target])\n",
    "            numeric_labels = np.array([0,1])\n",
    "\n",
    "            #   Retrieve numeric labels train\n",
    "            ship_labels = df['Target'].to_numpy()\n",
    "            stratify = []\n",
    "\n",
    "            for ship in ship_labels:\n",
    "                index = np.where(labels == ship)\n",
    "                stratify.append(index[0][0])\n",
    "\n",
    "            stratify = np.array(stratify)\n",
    "\n",
    "            y_train = stratify\n",
    "\n",
    "            #   Retrieve numeric labels test \n",
    "            ship_labels = df_test['Target'].to_numpy()\n",
    "            stratify = []\n",
    "\n",
    "            for ship in ship_labels:\n",
    "                index = np.where(labels == ship)\n",
    "                stratify.append(index[0][0])\n",
    "\n",
    "            stratify = np.array(stratify)\n",
    "\n",
    "            y_test = stratify\n",
    "\n",
    "\n",
    "            bst = XGBClassifier(n_estimators=100, max_depth=3, objective='binary:logistic', tree_method='gpu_hist')\n",
    "            np.save(f'{class_directory}y_train.npy', np.array(y_train))\n",
    "\n",
    "            # fit model\n",
    "            bst.fit(X_train, y_train)\n",
    "            bst.save_model(f'{class_directory}model.json')\n",
    "\n",
    "            \n",
    "            # both make predicition in probabilities and normal binary\n",
    "            y_pred_prob = bst.predict_proba(X_test)\n",
    "            y_pred_prob = np.array(y_pred_prob)\n",
    "            y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "            y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "            np.save(f'{class_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "            np.save(f'{class_directory}y_pred.npy', np.array(y_pred))\n",
    "            np.save(f'{class_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "            np.save(f'{class_directory}y_test.npy', np.array(y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPERIMENT Resampled sliding window\n",
    "def resampled_sliding(df_portcalls, results_directory):\n",
    "    # create necessary files for logging\n",
    "    current_results_directory = results_directory + \"experiment_resampled_sliding/\"\n",
    "    if not os.path.isdir(current_results_directory):\n",
    "        os.mkdir(current_results_directory)\n",
    "\n",
    "\n",
    "    # Retrieve the desired timeframe\n",
    "    df = get_time_frame_data(df_portcalls, START_DATE, END_DATE, \"ATA_LT\")\n",
    "\n",
    "\n",
    "    #   only keep relevant columns and set the id column\n",
    "    relevant_columns = {\"ATA_LT\": \"datetime64[ns]\", \n",
    "                        \"ATD_LT\": \"datetime64[ns]\",\n",
    "                        \"Port Name\": \"string\",\n",
    "                        \"IMO Number\": \"string\",\n",
    "                        \"(ATA) Ship Type Description\": \"string\"\n",
    "                        }\n",
    "    new_column_names = [\"Arrival Time\", \"Departure Time\", \"Port Name\", \"IMO number\", \"Ship Type\"]\n",
    "\n",
    "    id_column= \"IMO number\"\n",
    "\n",
    "    df = select_columns(df,relevant_columns=relevant_columns, new_column_names=new_column_names)\n",
    "\n",
    "    #   only keep the classes we want to classify\n",
    "    df = keep_classes(df, classes_to_keep=CLASSES_TO_USE, target_column='Ship Type', id_column='IMO number')\n",
    "   \n",
    "   \n",
    "\n",
    "    #   remove single occurences from network set\n",
    "    df_classify = remove_sinlge_occurences(df, column_name=\"IMO number\")\n",
    "\n",
    "\n",
    "    batch_size = relativedelta(months=1)\n",
    "    encoding_size = relativedelta(months=12)\n",
    "\n",
    "    \n",
    "    start_network = time.time()\n",
    "\n",
    "    current_run_log = current_results_directory + 'experiment_log.txt'\n",
    "\n",
    "    f = open(current_run_log, 'a')\n",
    "\n",
    "    df_train_batches = []\n",
    "    df_test_batches = []\n",
    "\n",
    "    end_date_train = datetime.datetime(2017, 6, 1)\n",
    "    end_date_test = datetime.datetime(2017, 7, 1, microsecond=1) \n",
    "\n",
    "    f.write(f'train batch 1:\\n')\n",
    "\n",
    "    test_sample = get_time_frame_data(df_classify, end_date_test- batch_size, end_date_test, \"Arrival Time\")\n",
    "    \n",
    "    test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.1)\n",
    "    \n",
    "    test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "    f.write(f'Amount of imos in test sample {len(test_imos)}\\n')        \n",
    "\n",
    "    test_batch = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_test - encoding_size, end_date=end_date_test)\n",
    "    f.write(f\"start date test: {end_date_test - encoding_size} and end date test: {end_date_test}\\n\")\n",
    "    test_imos = pd.unique(test_batch[\"IMO number\"])\n",
    "\n",
    "    train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                            start_date=end_date_train - batch_size, end_date=end_date_train)\n",
    "    \n",
    "    train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "    f.write(f\"start date train sample: {end_date_train - batch_size} and end date train sample: {end_date_train}\\n\")\n",
    "    f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "    \n",
    "    train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                            start_date=end_date_train - encoding_size, end_date=end_date_train)\n",
    "\n",
    "    train_imos = pd.unique(train_batch[\"IMO number\"])\n",
    "    network_imos = pd.unique(train_batch[\"IMO number\"])\n",
    "    f.write(f\"start date train: {end_date_train - encoding_size} and end date train: {end_date_train}\\n\")\n",
    "    f.write(f'there are {len(train_imos)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "    \n",
    "\n",
    "    overlap = np.intersect1d(train_imos, test_imos)\n",
    "    f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "    f.write('-----\\n')\n",
    "\n",
    "\n",
    "    G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(train_batch, id_column=\"IMO number\")\n",
    "    feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "    df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "    df_train_batches.append(df_features_train)\n",
    "\n",
    "    df_features_test = get_feature_df(test_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "    df_test_batches.append(df_features_test)\n",
    "\n",
    "    # exclude these imos for sampling test, to be consistent with static design\n",
    "    imos_to_exclude = network_imos\n",
    "\n",
    "    for x in range(40):\n",
    "        f.write(f'train batch {x+1}:\\n')\n",
    "        end_date_test += batch_size\n",
    "        end_date_train += batch_size \n",
    "        if end_date_test == datetime.datetime(2019, 12, 1, microsecond=1):\n",
    "            end_date_train = end_date_train + relativedelta(months=3)\n",
    "            end_date_test = end_date_test + relativedelta(months=3)\n",
    "        \n",
    "        if end_date_test >= END_DATE:\n",
    "            end_date_test = END_DATE\n",
    "\n",
    "        test_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=imos_to_exclude, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_test - batch_size, end_date=end_date_test)\n",
    "        test_sample, _ = split_network_train_set(df=test_sample, id_column=\"IMO number\", label_column=\"Ship Type\", network_size=0.3)\n",
    "        test_imos = pd.unique(test_sample[\"IMO number\"])\n",
    "\n",
    "        f.write(f'there are {len(test_imos)} imos sampled for test \\n')\n",
    "        \n",
    "        test_batch = get_subset_include_ids(df=df_classify, include_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_test - encoding_size, end_date=end_date_test)\n",
    "\n",
    "        test_imos_temp = pd.unique(test_batch[\"IMO number\"])\n",
    "        f.write(f\"start date test: {end_date_test - encoding_size} and end date test: {end_date_test}\\n\")\n",
    "        f.write(f'there are {len(test_imos_temp)} imos in test and {len(test_batch)} portcalls\\n')\n",
    "\n",
    "        train_sample = get_subset_exclude_ids(df=df_classify, exclude_ids=test_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_train - batch_size, end_date=end_date_train)\n",
    "        \n",
    "        train_imos = pd.unique(train_sample[\"IMO number\"])\n",
    "        f.write(f\"start date train sample: {end_date_train - batch_size} and end date train sample: {end_date_train}\\n\")\n",
    "        f.write(f'there are {len(train_imos)} imos sampled for train \\n')\n",
    "        \n",
    "        train_batch = get_subset_include_ids(df=df_classify, include_ids=train_imos, id_column=id_column, time_column=\"Arrival Time\",\n",
    "                                                start_date=end_date_train - encoding_size, end_date=end_date_train)\n",
    "        \n",
    "        train_imos = pd.unique(train_batch[\"IMO number\"])\n",
    "        network_imos = pd.unique(train_batch[\"IMO number\"])\n",
    "        f.write(f\"start date train: {end_date_train - encoding_size} and end date train: {end_date_train}\\n\")\n",
    "        f.write(f'there are {len(train_imos)} imos in train and {len(train_batch)} portcalls\\n')\n",
    "        \n",
    "\n",
    "        G, travel_times, port_stay_times, df_edges, processing_info = create_network_graph(train_batch, id_column=\"IMO number\")\n",
    "        feature_bins = create_feature_bins(G, travel_times=travel_times, port_stay_times=port_stay_times)\n",
    "\n",
    "        df_features_train = get_feature_df(train_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "        df_train_batches.append(df_features_train)\n",
    "\n",
    "        df_features_test = get_feature_df(test_batch, G, feature_bins=feature_bins, id_column=id_column, target_column='Ship Type')\n",
    "        df_test_batches.append(df_features_test)\n",
    "\n",
    "        overlap = np.intersect1d(train_imos, test_imos_temp)\n",
    "        f.write(f'there are {len(overlap)} imos overlapping between train and test\\n')\n",
    "        overlap_network = np.intersect1d(network_imos, test_imos_temp)\n",
    "        f.write(f'there are {len(overlap_network)} imos overlapping between network and test \\n')\n",
    "        f.write('-----\\n')\n",
    "\n",
    "\n",
    "    end_network = time.time()\n",
    "    dur_network = end_network - start_network\n",
    "    f.write(f\"In total from network creation until features took: {dur_network}, which is {dur_network/60} minutes\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Do the training and reporting\n",
    "    for i in range(len(df_train_batches)):\n",
    "        fold_directory = current_results_directory + f'/train_batch_{i+1}/'\n",
    "        if not os.path.isdir(fold_directory):\n",
    "            os.mkdir(fold_directory)\n",
    "\n",
    "        df_features_train = df_train_batches[i].copy()\n",
    "        test = df_test_batches[i].copy()\n",
    "\n",
    "        imo_train = df_features_train['IMO'].to_numpy()\n",
    "        imo_test = test['IMO'].to_numpy()\n",
    "\n",
    "        np.save(f'{fold_directory}imo_test.npy', np.array(imo_test))\n",
    "        np.save(f'{fold_directory}imo_train.npy', np.array(imo_train))\n",
    "    \n",
    "        for target in CLASSES_TO_USE:\n",
    "            name = target.replace('/', '-')\n",
    "            class_directory = fold_directory + f'/class_{name}/'\n",
    "            if not os.path.isdir(class_directory):\n",
    "                os.mkdir(class_directory)\n",
    "\n",
    "            df = df_features_train.copy()\n",
    "            df_test = test.copy()\n",
    "    \n",
    "            \n",
    "            #   transform every irrelevant classname to 'other'\n",
    "            df.loc[df['Target'] != target, 'Target'] = 'Other'\n",
    "            X_train = df.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "            df_test.loc[df_test['Target'] != target, 'Target'] = 'Other'\n",
    "            X_test = df_test.drop(['Target', 'IMO'], axis=1).to_numpy()\n",
    "\n",
    "\n",
    "            #   Retrieve a list of unique labels (both string and numeric)\n",
    "            labels = np.array([ 'Other', target])\n",
    "            numeric_labels = np.array([0,1])\n",
    "\n",
    "            #   Retrieve numeric labels train\n",
    "            ship_labels = df['Target'].to_numpy()\n",
    "            stratify = []\n",
    "\n",
    "            for ship in ship_labels:\n",
    "                index = np.where(labels == ship)\n",
    "                stratify.append(index[0][0])\n",
    "\n",
    "            stratify = np.array(stratify)\n",
    "\n",
    "            y_train = stratify\n",
    "\n",
    "            #   Retrieve numeric labels test \n",
    "            ship_labels = df_test['Target'].to_numpy()\n",
    "            stratify = []\n",
    "\n",
    "            for ship in ship_labels:\n",
    "                index = np.where(labels == ship)\n",
    "                stratify.append(index[0][0])\n",
    "\n",
    "            stratify = np.array(stratify)\n",
    "\n",
    "            y_test = stratify\n",
    "\n",
    "\n",
    "            bst = XGBClassifier(n_estimators=100, max_depth=3, objective='binary:logistic', tree_method='gpu_hist')\n",
    "            np.save(f'{class_directory}y_train.npy', np.array(y_train))\n",
    "\n",
    "            # fit model\n",
    "            bst.fit(X_train, y_train)\n",
    "            bst.save_model(f'{class_directory}model.json')\n",
    "\n",
    "            \n",
    "            # both make predicition in probabilities and normal binary\n",
    "            y_pred_prob = bst.predict_proba(X_test)\n",
    "            y_pred_prob = np.array(y_pred_prob)\n",
    "            y_pred_prob_roc = y_pred_prob[:, 1]\n",
    "            y_pred = np.argmax(y_pred_prob, axis= 1)\n",
    "            np.save(f'{class_directory}y_pred_prob.npy', np.array(y_pred_prob))\n",
    "            np.save(f'{class_directory}y_pred.npy', np.array(y_pred))\n",
    "            np.save(f'{class_directory}y_pred_prob_roc.npy', np.array(y_pred_prob_roc))\n",
    "            np.save(f'{class_directory}y_test.npy', np.array(y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
